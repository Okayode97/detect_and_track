{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Notebook to experiment with model optimizing model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import time\n",
    "from torchvision.models import detection\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSD(\n",
       "  (backbone): SSDLiteFeatureExtractorMobileNet(\n",
       "    (features): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): Conv2dNormActivation(\n",
       "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): ReLU()\n",
       "            (scale_activation): Hardsigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (extra): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], clip=True, scales=[0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.0], steps=None)\n",
       "  (head): SSDLiteHead(\n",
       "    (classification_head): SSDLiteClassificationHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(672, 546, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(480, 546, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(512, 546, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3-4): 2 x Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(256, 546, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(128, 546, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (regression_head): SSDLiteRegressionHead(\n",
       "      (module_list): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3-4): 2 x Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "            (2): ReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       "      Resize(min_size=(320,), max_size=320, mode='bilinear')\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load baseline model for optimization and benchmarking...\n",
    "baseline_model = detection.ssdlite320_mobilenet_v3_large(weights=detection.SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)\n",
    "baseline_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 2 threads\n"
     ]
    }
   ],
   "source": [
    "# benchmark basic model.\n",
    "num_threads = torch.get_num_threads()\n",
    "print(f\"Benchmarking on {num_threads} threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Baseline Model\n",
    "- Attempt: Benchmark the baseline model and try using reducing image size used to benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import baseline_model\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "\n",
    "baseline_benchmark = benchmark.Timer(\n",
    "    stmt = \"baseline_model(x)\",\n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"Baseline model\")\n",
    "\n",
    "print(baseline_benchmark.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_resolution_setup = '''\n",
    "import torch\n",
    "from __main__ import baseline_model\n",
    "\n",
    "x = torch.rand(1, 3, 320, 240)\n",
    "'''\n",
    "\n",
    "baseline_benchmark_reduced_resolution =  benchmark.Timer(\n",
    "    stmt = \"baseline_model(x)\",\n",
    "    setup= reduced_resolution_setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"Baseline model\",\n",
    "    sub_label=\"reduced resolution\")\n",
    "\n",
    "print(baseline_benchmark_reduced_resolution.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Scripting\n",
    "general points to note scripting and tracing applies optimization to the model to improves inference speed in production environment. Tracing the model effectively freezes the conditional logic of the model to match the data given during tracing. There are more subtle difference between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply torch scripitng...\n",
    "scripted_model = torch.jit.script(baseline_model.eval())\n",
    "scripted_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark the performance of the model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import scripted_model\n",
    "\n",
    "x = torch.rand(3, 640, 480)\n",
    "'''\n",
    "scripted_model_benchmark = benchmark.Timer(\n",
    "    stmt = \"scripted_model([x])\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"Scripted baseline model\")\n",
    "\n",
    "print(scripted_model_benchmark.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripting the model results in some improvement in time taken to run inference, the resulting speedup are not consistent with multiple run. As a sidenote i've also tried reducing the resolution of the input image passed to the model to see if it improves inference time, it doesn't seem to result in any significant speed up. The results reported above where obtained after briefly going through notes on torchscripting there might be more details/approach to scripting and tracing that might improve inference time.\n",
    "\n",
    "i'll try other approaches\n",
    "- quantizations\n",
    "- experimenting with onnx runtime\n",
    "\n",
    "https://www.reddit.com/r/MachineLearning/comments/yg1mpz/d_how_to_get_the_fastest_pytorch_inference_and/\n",
    "\n",
    "As a side note or maybe a more concluding thought on scripting as an optimization approach.   \n",
    "Scripted model main advantage is that they can also run independently of a python environment, So they are designed to be flexible and portable, enabling deployment into a non-python environment. They are not necessarily an optimization technique to speed up inference, it's more focused on flexible and portable deployments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "(More of a summary of notes in model optimization.md)\n",
    "\n",
    "Quantization is an optimization technique that can be applied to a model to reduce it's size and increase it's inference speed (with the caveat that we have *about* the same accuracy as the original model). Quantization at a glance operates by converting high precision numbers into lower precision, the result is a reduction in model size and computational cost, with some negliable loss in accuracy. Three [approaches](https://pytorch.org/tutorials/recipes/quantization.html) to quantization are\n",
    "- Post training Dynamic Quantization (converts model weight to 8bit integers but does not convert activation untill it's used to compute further activation. At time of writing this method only supports nn.Linear and nn.LSTM modules to apply quantization to.)\n",
    "- Post training Static Quantization (Converts both weights and activation to 8 bit integers, there is no on-the-fly conversion of the model activation during inference)\n",
    "- Quantization aware training (from what i can gather this approach involves quantizing/dequantizing the inputs and activation of the model, the conversion is baked into the model architecture and used as part of model training..)\n",
    "\n",
    "\n",
    "for my application is it worth checking if there already exist a quantized version of the detection models?? There are quantized versions of the backbone but there are no quantized versions of the detection models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.quantized.supported_engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post training static quantization\n",
    "backend = \"qnnpack\"\n",
    "\n",
    "baseline_model = baseline_model.to(\"cpu\")\n",
    "\n",
    "baseline_model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized = torch.quantization.prepare(baseline_model, inplace=False)\n",
    "model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running inference on quantized model\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "\n",
    "quant = torch.ao.quantization.QuantStub()\n",
    "x_quantized = quant(x)\n",
    "prediction = model_static_quantized(x_quantized)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "understanding *Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build).*\n",
    "looking at this [post](https://discuss.pytorch.org/t/error-in-running-quantised-model-runtimeerror-could-not-run-quantized-conv2d-new-with-arguments-from-the-cpu-backend/151718) it seems that you have to quanitize the input passed to the quantized model, and the approach is use `QuantStub` and `DeQuantStub` function to quantize and dequantize the input. Also it seems a different approach to quantizating the specific layers in the model was introduced (the different approach seems to be more related to quantization aware training), it might be worth trying with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap model input and output, quantizing input and de-quantizing the output\n",
    "class QuantizedModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model_fp32 = model\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize model with inputs wrapped with quant & dequant function\n",
    "quantized_model = QuantizedModel(baseline_model)\n",
    "\n",
    "backend = \"fbgemm\"\n",
    "baseline_model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized = torch.quantization.prepare(quantized_model, inplace=False)\n",
    "model_static_quantized = torch.quantization.convert(quantized_model, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 3, 640, 480)\n",
    "\n",
    "# quant = torch.ao.quantization.QuantStub()\n",
    "# x_quantized = quant(x)\n",
    "prediction = model_static_quantized(x)\n",
    "print(prediction)\n",
    "\n",
    "# IT WORKS !!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried simply quantizing the inputs before running the model forward function, but runing that did not resolve the issue, but wrapping up the forward function for the quantized model class did the trick. Does that mean the quantization stubs need to be within the forward function of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark static quantized model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import model_static_quantized\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "quantized_model = benchmark.Timer(\n",
    "    stmt = \"model_static_quantized(x)\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"quantized model\")\n",
    "\n",
    "print(quantized_model.timeit(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resulting quantized model is not as fast as i would have thought. There are some improvements but the inference time is not consistent with multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there a difference in quantizing the model with different backend\n",
    "# available backend ['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n",
    "\n",
    "def quantize_model_with_different_backend(model: torch.nn.Module, backend: str):\n",
    "\n",
    "    quantized_model = QuantizedModel(model.to(\"cpu\")) # not really quantized, but it's inputs and outputs have been wrapped with quantization stub\n",
    "\n",
    "    quantized_model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "    torch.backends.quantized.engine = backend\n",
    "    model_static_quantized = torch.quantization.prepare(quantized_model, inplace=False)\n",
    "    model_static_quantized = torch.quantization.convert(quantized_model, inplace=False)\n",
    "    \n",
    "\n",
    "    return model_static_quantized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantizing with different backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qnnpack model\n",
    "qnn_pack_backend_quantized_model = quantize_model_with_different_backend(baseline_model, \"qnnpack\")\n",
    "\n",
    "# benchmark static quantized model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import qnn_pack_backend_quantized_model\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "quantized_qnnpack_model = benchmark.Timer(\n",
    "    stmt = \"qnn_pack_backend_quantized_model(x)\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"quantized qnnpack model\")\n",
    "\n",
    "print(quantized_qnnpack_model.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onednn backend\n",
    "onednn_backend_quantized_model = quantize_model_with_different_backend(baseline_model, \"onednn\")\n",
    "\n",
    "# benchmark static quantized model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import onednn_backend_quantized_model\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "quantized_onednn_model = benchmark.Timer(\n",
    "    stmt = \"onednn_backend_quantized_model(x)\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"quantized onednn model\")\n",
    "\n",
    "print(quantized_onednn_model.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x86 backend\n",
    "x86_backend_quantized_model = quantize_model_with_different_backend(baseline_model, \"x86\")\n",
    "\n",
    "# benchmark static quantized model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import x86_backend_quantized_model\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "quantized_x86_model = benchmark.Timer(\n",
    "    stmt = \"x86_backend_quantized_model(x)\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"quantized x86 model\")\n",
    "\n",
    "print(quantized_x86_model.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try fusing model layers\n",
    "\n",
    "**Quantization backend Significance**   \n",
    "significance of different backend, which backend to use?   \n",
    "The different backends specfiy how quantized operations are implemented and optimized for specific hardware platform.   \n",
    "- `fbgem` backend, is `optimized for x86 platforms such as intels CPU`. It uses efficient algorithms and instructions specific to x86 hardware to maximize performance. It's optimized for x86 server CPU and is widely used in data center environment where intel or AMD x86 processors are predominant.\n",
    "\n",
    "- `qnnpack` is `optimized for mobile devices` such as ARM CPUs and is designed to offer efficient performance on smartphones and tablets, making it suitable for mobile applications. qnnpack supports 8-bit integer quantization and provides optimizations for mobile inference.\n",
    "\n",
    "- `onednn` is an `intel-developed library that provides optimization for both CPU and GPU`. It's used for server-side applications where intel hardware is predominant. It can be useful for both quantized and floating point compuations. Also worth noting that `onednn` is `focused on x86 intel platforms`, the key difference between it and `fbgem` is that it's `more versatile in that it provides optimization for both CPU and GPU`\n",
    "\n",
    "Given that i've switched from running the model locally on the raspberry pi to hosting it on a fastapi server on an old linux server, i can't use qnnpack backend so that leaves `x86`, `onednn`, `fbgem`, also looking at the benchmarking results with the model quantized with qnnpack, had the longest inference time. With fbgem and x86 giving the *best reduction in inference time* \n",
    "\n",
    "note.   \n",
    "- checking linux laptop: running `uname -m` gave results x86-64, querying on results indicated that laptop is running a 64-bit version of the x86 architecture, which is also known as \"AMD64\" or \"Intel 64.\" \n",
    "\n",
    "\n",
    "One last try, with **fusing model layers**\n",
    "**Try fusing model layers**\n",
    "Notes from [quantization in practice](https://pytorch.org/blog/quantization-in-practice/#backend-engine)\n",
    "- Post training static quantization:\n",
    "    - quantizes the model weightts and activation, as opposed to applying on-the-fly quantization for the activations. The activation would stay in a quantized precision between operations during inference.\n",
    "    - Additional methods to apply into post training static quantization workflow. Fuse modules, get calibration data and calibrate the model before applying quantization.\n",
    "\n",
    "*\"Module fusion combines multiple sequential modules into one. Fusing modules mean the compiler needs to only run one kernel instead of many; this speeds things up and improve accuracy by reducing quantization error\"* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import fuse_modules\n",
    "import copy\n",
    "\n",
    "\n",
    "baseline_model_copy = copy.deepcopy(baseline_model)\n",
    "for name, module in baseline_model_copy.named_children():\n",
    "    print(name)\n",
    "    print(\"-------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in baseline_model_copy.named_children():\n",
    "    print(name, module)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the backbone and fuse first two elements in conv2d norm activation block\n",
    "def find_all_conv2d_norm_activation_blocks_and_fuse_them(model):\n",
    "\n",
    "    def recurse_submodules(modules):\n",
    "        for name, sub_module in modules.named_children():\n",
    "            if type(sub_module) == torch.nn.modules.container.Sequential:\n",
    "               for layer in sub_module:\n",
    "                   if type(layer).__name__ == 'Conv2dNormActivation':\n",
    "                        # if type(layer[1]) == torch.nn.modules.batchnorm.BatchNorm2d:\n",
    "                        #     fuse_modules(layer, [\"0\", \"1\"], inplace=True)\n",
    "                        print(layer)\n",
    "                   else:\n",
    "                        recurse_submodules(sub_module)\n",
    "            else:\n",
    "                if type(sub_module).__name__ == 'Conv2dNormActivation':\n",
    "                    # if type(sub_module[1]) == torch.nn.modules.batchnorm.BatchNorm2d:\n",
    "                    #     fuse_modules(sub_module, [\"0\", \"1\"], inplace=True)\n",
    "                    print(sub_module)\n",
    "                else:\n",
    "                    recurse_submodules(sub_module)\n",
    "\n",
    "    recurse_submodules(model)\n",
    "\n",
    "find_all_conv2d_norm_activation_blocks_and_fuse_them(baseline_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model\n",
    "fbgem_model_copy = copy.deepcopy(baseline_model_copy)\n",
    "fbgemm_backend_quantized_model = quantize_model_with_different_backend(fbgem_model_copy, \"fbgemm\")\n",
    "\n",
    "# benchmark static quantized model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import fbgemm_backend_quantized_model\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "quantized_fbgemm_model_benchmark = benchmark.Timer(\n",
    "    stmt = \"fbgemm_backend_quantized_model(x)\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"quantized fbgemm model\")\n",
    "\n",
    "print(quantized_fbgemm_model_benchmark.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x86_model_copy = copy.deepcopy(baseline_model_copy)\n",
    "x86_backend_quantized_model = quantize_model_with_different_backend(x86_model_copy, \"x86\")\n",
    "\n",
    "# benchmark static quantized model\n",
    "setup = '''\n",
    "import torch\n",
    "from __main__ import x86_backend_quantized_model\n",
    "\n",
    "x = torch.rand(1, 3, 640, 480)\n",
    "'''\n",
    "quantized_x86_model_benchmark = benchmark.Timer(\n",
    "    stmt = \"x86_backend_quantized_model(x)\", # interesting note that i've add to pass the \n",
    "    setup= setup,\n",
    "    num_threads=num_threads,\n",
    "    label=\"quantized x86 model\")\n",
    "\n",
    "print(quantized_x86_model_benchmark.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some minor speed up in inference but, it does not seem to be consistent or significant. I can spend more effort looking into other optimization techniques available on pytorch, there most likely is something i'm missing, i'll put the quantization and pytorch optimization techniques in the backlog for now and look into Onnx & OpenVino.\n",
    "\n",
    "Notes on what else i can try   \n",
    "- Look into quantization aware training.\n",
    "- Train custom and more lightweight model for object detection (requires collection of data and labelling effort).\n",
    "- Try calibrating model before applying quantization.\n",
    "- Use classifers with quantized weights and retrain for object detection.\n",
    "- Look into other model sources or ML platform (Tensorflow?)\n",
    "- Research some more/ ask pytorch forum for further guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onnx\n",
    "\n",
    "Onnx (Open Neural Network Exchange), is an open standard format, desgined to represent machine learning models. It facilitates the sharing and deployment of models across various frameworks, platforms and tools. Key features of Onnx\n",
    "\n",
    "- It enables models to be tained in one framework and then deployed in another\n",
    "- It works with many existing model frameworks and using it's own native runtime we can optimize and execute models efficiently.\n",
    "- It provides optimizers to imporve performance on target hardware or specialised accelerators.\n",
    "\n",
    "Onnx's ecosystem includes\n",
    "- a runtime inference engines,\n",
    "- a model zoo consisting of a collection of pre-trained models in Onnx format\n",
    "- a model converter which converts model into the Onnx format.\n",
    "\n",
    "Typical use cases for Onnx includes model deployment, optimization and portability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install onnxruntime and onnx (docs claims it should be installed with pytorch)\n",
    "# convert the baseline model into onnx format\n",
    "torch.onnx.export(baseline_model,\n",
    "                  torch.rand(1, 3, 640, 480).to(\"cpu\"),\n",
    "                  \"baseline_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the onnx model\n",
    "import onnx\n",
    "\n",
    "baseline_model_onnx = onnx.load(\"baseline_model.onnx\")\n",
    "onnx.checker.check_model(baseline_model_onnx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 12.629723 ,  16.061035 , 470.73523  , 629.4593   ],\n",
      "       [ 11.8641815,  14.589386 , 471.1045   , 630.06995  ],\n",
      "       [ 13.892509 ,   3.1773682, 469.0436   , 638.76     ],\n",
      "       ...,\n",
      "       [435.1275   ,  64.93329  , 453.64447  ,  93.64925  ],\n",
      "       [413.16055  ,  52.537483 , 450.42133  ,  98.12908  ],\n",
      "       [374.31528  , 321.45984  , 391.1797   , 353.82904  ]],\n",
      "      dtype=float32), array([0.05734642, 0.03076115, 0.02796767, 0.02418317, 0.02380787,\n",
      "       0.02320855, 0.02276548, 0.02227207, 0.02121965, 0.02090928,\n",
      "       0.02090814, 0.02069603, 0.02061966, 0.01978374, 0.01970458,\n",
      "       0.01952676, 0.01912172, 0.01906435, 0.01887483, 0.01878311,\n",
      "       0.01864178, 0.01831578, 0.0182779 , 0.01819321, 0.01763916,\n",
      "       0.01762065, 0.01730298, 0.01702949, 0.01693076, 0.01687463,\n",
      "       0.0168498 , 0.01670376, 0.01670239, 0.01667927, 0.0166126 ,\n",
      "       0.01647821, 0.01642449, 0.01641004, 0.01631321, 0.01630311,\n",
      "       0.0161071 , 0.01598804, 0.01597957, 0.01591544, 0.015883  ,\n",
      "       0.01585099, 0.0158025 , 0.01572404, 0.0156942 , 0.01565717,\n",
      "       0.01565594, 0.01550851, 0.01548769, 0.01548336, 0.0154722 ,\n",
      "       0.01537467, 0.01529732, 0.01523694, 0.01523649, 0.01516254,\n",
      "       0.01515439, 0.01513044, 0.01509509, 0.01508988, 0.01504613,\n",
      "       0.01502165, 0.01494927, 0.01492799, 0.01489101, 0.0147538 ,\n",
      "       0.01475329, 0.01470302, 0.01468875, 0.01466376, 0.01466232,\n",
      "       0.01459741, 0.01457179, 0.01455948, 0.01455477, 0.01437386,\n",
      "       0.01435092, 0.0142149 , 0.01417794, 0.01417508, 0.01414972,\n",
      "       0.01411116, 0.01411029, 0.01408304, 0.01407349, 0.01406673,\n",
      "       0.01403933, 0.01403779, 0.01400854, 0.01398296, 0.01398018,\n",
      "       0.0139799 , 0.01397455, 0.01396482, 0.01395653, 0.01393027,\n",
      "       0.01386301, 0.01385625, 0.01384833, 0.0137721 , 0.01377084,\n",
      "       0.01376639, 0.01375754, 0.01375252, 0.01375243, 0.01368227,\n",
      "       0.01367981, 0.01364476, 0.01363493, 0.0136105 , 0.01360789,\n",
      "       0.01357844, 0.01357757, 0.01355433, 0.01354963, 0.01350792,\n",
      "       0.01346115, 0.013452  , 0.01344596, 0.01343892, 0.01340703,\n",
      "       0.01339145, 0.0133855 , 0.01331535, 0.01331283, 0.01329713,\n",
      "       0.01328092, 0.01327897, 0.01326874, 0.01317608, 0.01315827,\n",
      "       0.01313929, 0.01312015, 0.01311276, 0.01302975, 0.01302756,\n",
      "       0.01302493, 0.01301382, 0.01301053, 0.01300857, 0.01300361,\n",
      "       0.01295335, 0.01294578, 0.01291255, 0.01289131, 0.01285372,\n",
      "       0.01282402, 0.01282296, 0.01281051, 0.01279953, 0.01278771,\n",
      "       0.0127492 , 0.01274736, 0.01274366, 0.01272166, 0.01270413,\n",
      "       0.01269726, 0.01269287, 0.01268381, 0.01267432, 0.01263013,\n",
      "       0.01259906, 0.01257951, 0.01253704, 0.0125296 , 0.01252692,\n",
      "       0.01252661, 0.01251163, 0.01248497, 0.01248439, 0.01246106,\n",
      "       0.0124463 , 0.01240485, 0.01240389, 0.0124021 , 0.01238092,\n",
      "       0.01237   , 0.01235211, 0.01234914, 0.01234384, 0.01233729,\n",
      "       0.01232964, 0.0123207 , 0.0123045 , 0.01228121, 0.01226378,\n",
      "       0.0122186 , 0.01221817, 0.0121939 , 0.01219054, 0.01217704,\n",
      "       0.01217336, 0.01216856, 0.01216443, 0.01215094, 0.01213203,\n",
      "       0.0121021 , 0.01209125, 0.01207082, 0.01205834, 0.01202401,\n",
      "       0.01202338, 0.01201044, 0.01199091, 0.01197356, 0.01196709,\n",
      "       0.01196356, 0.01195956, 0.01195309, 0.01192719, 0.01191188,\n",
      "       0.01190143, 0.01190035, 0.01190035, 0.01186469, 0.01186037,\n",
      "       0.01183346, 0.01181929, 0.01179782, 0.0117834 , 0.01177226,\n",
      "       0.01174996, 0.01174771, 0.01173795, 0.01172583, 0.01171548,\n",
      "       0.01171483, 0.01168909, 0.01168462, 0.01168157, 0.01167738,\n",
      "       0.01164814, 0.01163328, 0.01161505, 0.01160822, 0.01159714,\n",
      "       0.01159521, 0.01159282, 0.01159098, 0.01158975, 0.01158918,\n",
      "       0.01158606, 0.01156865, 0.01156252, 0.01155686, 0.01155626,\n",
      "       0.01155286, 0.01154119, 0.01152092, 0.01152092, 0.01150503,\n",
      "       0.01150353, 0.01150117, 0.01149363, 0.01144461, 0.0114434 ,\n",
      "       0.01142529, 0.01142162, 0.01142118, 0.01140246, 0.01139171,\n",
      "       0.01138095, 0.01138036, 0.01137662, 0.01137659, 0.01136961,\n",
      "       0.01136743, 0.01136578, 0.01135549, 0.01134023, 0.01134012,\n",
      "       0.0113393 , 0.01133747, 0.01130041, 0.0112955 , 0.01128699,\n",
      "       0.01127075, 0.01126886, 0.01124972, 0.01124863, 0.01124595,\n",
      "       0.01124244, 0.0112406 , 0.01123663, 0.01123577, 0.0112344 ,\n",
      "       0.01123393, 0.01122517, 0.01122434, 0.01122288, 0.0112226 ,\n",
      "       0.01122157, 0.01121528, 0.01120792, 0.01120498, 0.01119633],\n",
      "      dtype=float32), array([ 1, 63, 67, 16, 16,  1,  1, 16, 16, 16,  1,  1, 16, 16, 65,  1,  1,\n",
      "       16, 16, 16, 16, 16,  1, 16, 16, 16, 16, 16,  1, 28, 16,  1,  1, 16,\n",
      "        1, 16,  1, 16, 16, 16, 16,  1, 15,  1, 16,  1, 16, 16,  1, 16, 16,\n",
      "       16, 16,  1, 16, 67, 16,  1,  1, 16, 16, 16, 16, 16, 16,  1, 38,  1,\n",
      "        1, 16, 16, 16,  1, 16,  1, 16, 16,  1, 16, 16, 16,  1,  1, 16, 16,\n",
      "       16,  1, 16,  1, 16,  1, 16, 16, 16, 38,  1,  1, 16, 16,  1, 16,  1,\n",
      "        1,  1, 16, 16, 16, 16, 16,  1, 16, 16, 16, 67, 16, 16,  1, 16, 38,\n",
      "       38, 16, 38, 16, 16, 16,  1, 16, 16,  1, 16, 16, 38, 16, 16,  1,  1,\n",
      "       16, 38, 16,  1, 16, 16, 16, 38, 16,  1, 16,  1, 16, 16,  1,  1, 16,\n",
      "        1,  1, 16, 16, 62, 16, 16, 16,  1,  1, 16, 38, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16,  1,  1, 16, 16, 16,  1, 16, 38,  1, 38, 38,  1, 38,\n",
      "        1,  1, 38, 16, 16, 38, 16,  1, 38,  1, 16, 16,  1, 38, 16, 16, 16,\n",
      "       16, 38, 16, 16,  1,  1, 16, 16, 16, 16, 38, 16, 38,  1, 16,  1, 16,\n",
      "       16,  1,  1,  1, 38, 16, 38, 37,  1,  1,  1,  1, 16,  1, 67, 38, 38,\n",
      "       16, 16,  1, 38, 38, 38, 16, 38, 16,  1, 16,  1, 16, 16, 38,  1, 16,\n",
      "       16, 38, 16, 38, 16, 16,  1,  1, 38,  1, 16,  1, 16, 16, 38, 38, 16,\n",
      "        1, 16, 38, 38, 16, 16, 16,  1, 38, 16,  1, 38, 43, 16, 16, 16, 16,\n",
      "       38, 16, 16,  1,  1,  1, 16, 16, 16, 16, 16], dtype=int64)]\n"
     ]
    }
   ],
   "source": [
    "# create an inference session and run the model using random input\n",
    "import onnxruntime as ort\n",
    "\n",
    "random_input = torch.rand(1, 3, 640, 480).to(\"cpu\")\n",
    "ort_sess = ort.InferenceSession(\"baseline_model.onnx\")\n",
    "output = ort_sess.run(None, {\"images\": random_input.numpy()})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the onnx model against the original baseline model\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
